"""
================================================================================
TRANSFORMER-BASED SEQUENCE MODEL
================================================================================
State-of-the-Art Deep Learning for Cryptocurrency Prediction

Architecture: Temporal Fusion Transformer (TFT) inspired
- Multi-head self-attention for long-range dependencies
- Gated residual networks for feature selection
- Multi-horizon quantile forecasting

Paper Reference: "Temporal Fusion Transformers for Interpretable 
Multi-horizon Time Series Forecasting" (Lim et al., 2019)
================================================================================
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import json


@dataclass
class TransformerConfig:
    """Configuration for Transformer model"""
    # Model architecture
    d_model: int = 64  # Embedding dimension
    n_heads: int = 4   # Attention heads
    n_encoder_layers: int = 2
    n_decoder_layers: int = 2
    d_ff: int = 256    # Feed-forward dimension
    dropout: float = 0.1
    
    # Sequence parameters
    lookback_window: int = 48  # 8 days of 4h bars
    forecast_horizon: int = 1   # 4-hour ahead
    
    # Feature parameters
    max_features: int = 32
    use_position_embedding: bool = True
    use_temporal_embedding: bool = True


class TemporalAttention:
    """
    Scaled Dot-Product Attention with Causal Masking
    """
    
    def __init__(self, d_model: int, n_heads: int):
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        # Initialize weights (simplified - in practice use proper init)
        self.W_q = np.random.randn(d_model, d_model) * 0.02
        self.W_k = np.random.randn(d_model, d_model) * 0.02
        self.W_v = np.random.randn(d_model, d_model) * 0.02
        self.W_o = np.random.randn(d_model, d_model) * 0.02
        
    def softmax(self, x: np.ndarray) -> np.ndarray:
        """Numerically stable softmax"""
        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)
    
    def attention(self, Q: np.ndarray, K: np.ndarray, V: np.ndarray, 
                  mask: Optional[np.ndarray] = None) -> Tuple[np.ndarray, np.ndarray]:
        """
        Scaled dot-product attention
        
        Args:
            Q: Query matrix [batch, seq_len, d_model]
            K: Key matrix [batch, seq_len, d_model]
            V: Value matrix [batch, seq_len, d_model]
            mask: Causal mask for autoregressive property
            
        Returns:
            output: Attention output
            attention_weights: For interpretability
        """
        scores = np.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)
        
        if mask is not None:
            scores = np.where(mask, scores, -1e9)
        
        attention_weights = self.softmax(scores)
        output = np.matmul(attention_weights, V)
        
        return output, attention_weights
    
    def forward(self, x: np.ndarray, mask: Optional[np.ndarray] = None) -> np.ndarray:
        """
        Multi-head attention forward pass
        
        Args:
            x: Input [seq_len, d_model]
            
        Returns:
            output: [seq_len, d_model]
        """
        seq_len = x.shape[0]
        
        # Linear projections
        Q = np.dot(x, self.W_q)
        K = np.dot(x, self.W_k)
        V = np.dot(x, self.W_v)
        
        # Reshape for multi-head: [seq_len, n_heads, d_k]
        Q = Q.reshape(seq_len, self.n_heads, self.d_k)
        K = K.reshape(seq_len, self.n_heads, self.d_k)
        V = V.reshape(seq_len, self.n_heads, self.d_k)
        
        # Transpose for attention: [n_heads, seq_len, d_k]
        Q = Q.transpose(1, 0, 2)
        K = K.transpose(1, 0, 2)
        V = V.transpose(1, 0, 2)
        
        # Apply attention
        attn_output, attn_weights = self.attention(Q, K, V, mask)
        
        # Concatenate heads
        attn_output = attn_output.transpose(1, 0, 2).reshape(seq_len, self.d_model)
        
        # Output projection
        output = np.dot(attn_output, self.W_o)
        
        return output


class PositionalEncoding:
    """Sinusoidal positional encoding"""
    
    def __init__(self, d_model: int, max_len: int = 5000):
        self.d_model = d_model
        
        # Create positional encoding matrix
        position = np.arange(max_len).reshape(-1, 1)
        div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))
        
        pe = np.zeros((max_len, d_model))
        pe[:, 0::2] = np.sin(position * div_term)
        pe[:, 1::2] = np.cos(position * div_term)
        
        self.pe = pe
    
    def encode(self, seq_len: int) -> np.ndarray:
        """Get positional encoding for sequence length"""
        return self.pe[:seq_len]


class LayerNorm:
    """Layer normalization"""
    
    def __init__(self, features: int, eps: float = 1e-6):
        self.eps = eps
        self.gamma = np.ones(features)
        self.beta = np.zeros(features)
    
    def normalize(self, x: np.ndarray) -> np.ndarray:
        mean = np.mean(x, axis=-1, keepdims=True)
        std = np.std(x, axis=-1, keepdims=True)
        return self.gamma * (x - mean) / (std + self.eps) + self.beta


class FeedForward:
    """Position-wise feed-forward network"""
    
    def __init__(self, d_model: int, d_ff: int):
        self.W_1 = np.random.randn(d_model, d_ff) * 0.02
        self.b_1 = np.zeros(d_ff)
        self.W_2 = np.random.randn(d_ff, d_model) * 0.02
        self.b_2 = np.zeros(d_model)
    
    def forward(self, x: np.ndarray) -> np.ndarray:
        """FFN(x) = max(0, xW1 + b1)W2 + b2"""
        hidden = np.maximum(0, np.dot(x, self.W_1) + self.b_1)  # ReLU
        return np.dot(hidden, self.W_2) + self.b_2


class GatedResidualNetwork:
    """
    Gated Residual Network for feature selection
    Key component of TFT architecture
    """
    
    def __init__(self, input_size: int, hidden_size: int, output_size: int, 
                 dropout: float = 0.1):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        
        # Weights
        self.W_1 = np.random.randn(input_size, hidden_size) * 0.02
        self.b_1 = np.zeros(hidden_size)
        self.W_2 = np.random.randn(hidden_size, output_size) * 0.02
        self.b_2 = np.zeros(output_size)
        
        # Gating mechanism
        self.W_gate = np.random.randn(input_size, output_size) * 0.02
        self.b_gate = np.zeros(output_size)
        
        # Skip connection projection if needed
        if input_size != output_size:
            self.W_skip = np.random.randn(input_size, output_size) * 0.02
        else:
            self.W_skip = None
        
        self.dropout = dropout
        self.layer_norm = LayerNorm(output_size)
    
    def forward(self, x: np.ndarray, context: Optional[np.ndarray] = None) -> np.ndarray:
        """
        GRN forward with gating mechanism
        
        Args:
            x: Input features
            context: Optional context vector for conditioning
        """
        # Main pathway
        hidden = np.dot(x, self.W_1) + self.b_1
        hidden = np.maximum(0, hidden)  # ELU activation simplified
        
        if context is not None:
            hidden = hidden + context
        
        hidden = np.dot(hidden, self.W_2) + self.b_2
        
        # Gating
        gate = 1 / (1 + np.exp(-(np.dot(x, self.W_gate) + self.b_gate)))  # Sigmoid
        
        # Skip connection
        if self.W_skip is not None:
            skip = np.dot(x, self.W_skip)
        else:
            skip = x
        
        # Gated output
        output = gate * hidden + (1 - gate) * skip
        
        # Layer norm
        output = self.layer_norm.normalize(output)
        
        return output


class TransformerPredictor:
    """
    Transformer-based cryptocurrency prediction model
    
    Features:
    - Multi-head self-attention for capturing long-range dependencies
    - Variable selection networks for feature importance
    - Quantile forecasting for uncertainty estimation
    - Interpretable attention weights
    """
    
    def __init__(self, config: TransformerConfig = None):
        self.config = config or TransformerConfig()
        
        # Initialize components
        self.pos_encoder = PositionalEncoding(self.config.d_model, 
                                               self.config.lookback_window)
        self.attention = TemporalAttention(self.config.d_model, 
                                           self.config.n_heads)
        self.layer_norm = LayerNorm(self.config.d_model)
        self.ffn = FeedForward(self.config.d_model, self.config.d_ff)
        
        # Variable selection networks
        self.vsn_grn = GatedResidualNetwork(
            self.config.max_features,
            self.config.d_model,
            self.config.d_model
        )
        
        # Output layers for quantile forecasting
        self.quantiles = [0.1, 0.5, 0.9]  # P10, P50, P90
        self.output_weights = {
            q: np.random.randn(self.config.d_model, 1) * 0.02
            for q in self.quantiles
        }
        
        # Temporal attention weights for interpretability
        self.attention_weights_history = []
    
    def prepare_features(self, df: pd.DataFrame) -> np.ndarray:
        """
        Prepare feature matrix from dataframe
        
        Creates normalized feature set suitable for transformer input
        """
        features = []
        
        # Price-based features
        returns = df['price'].pct_change().fillna(0)
        features.append(returns)
        
        # Multi-scale returns
        for window in [6, 12, 24, 48]:
            features.append(returns.rolling(window).mean().fillna(0))
        
        # Volatility features
        for window in [12, 24, 48]:
            vol = returns.rolling(window).std().fillna(0) * np.sqrt(2190)
            features.append(vol)
        
        # Volume features if available
        if 'volume' in df.columns:
            vol_norm = (df['volume'] / df['volume'].rolling(24).mean()).fillna(1)
            features.append(vol_norm)
            features.append(returns * vol_norm)  # Volume-weighted returns
        
        # Technical indicators
        # RSI proxy
        delta = df['price'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
        rs = gain / loss.replace(0, 1)
        rsi = 1 - (1 / (1 + rs))
        features.append(rsi.fillna(0.5))
        
        # MACD proxy
        ema_fast = df['price'].ewm(span=12).mean()
        ema_slow = df['price'].ewm(span=26).mean()
        macd = (ema_fast - ema_slow) / df['price']
        features.append(macd.fillna(0))
        
        # Trend strength
        sma_ratio = df['price'] / df['price'].rolling(20).mean() - 1
        features.append(sma_ratio.fillna(0))
        
        # Stack and normalize
        feature_matrix = np.column_stack([f.values[-self.config.lookback_window:] 
                                          for f in features[:self.config.max_features]])
        
        # Z-score normalization per feature
        feature_matrix = (feature_matrix - np.mean(feature_matrix, axis=0)) / \
                         (np.std(feature_matrix, axis=0) + 1e-8)
        
        return feature_matrix[-self.config.lookback_window:]
    
    def predict(self, df: pd.DataFrame) -> Dict:
        """
        Generate prediction using transformer architecture
        
        Returns quantile forecasts and attention-based interpretability
        """
        # Prepare input
        x = self.prepare_features(df)
        seq_len = x.shape[0]
        
        # Variable selection
        selected_features = self.vsn_grn.forward(x)
        
        # Add positional encoding
        if self.config.use_position_embedding:
            pos_enc = self.pos_encoder.encode(seq_len)
            selected_features = selected_features + pos_enc
        
        # Multi-head self-attention
        # Create causal mask
        mask = np.tril(np.ones((seq_len, seq_len)))
        
        attn_output = self.attention.forward(selected_features, mask)
        
        # Residual connection and layer norm
        attn_output = self.layer_norm.normalize(selected_features + attn_output)
        
        # Feed-forward network
        ffn_output = self.ffn.forward(attn_output)
        
        # Final residual
        output = self.layer_norm.normalize(attn_output + ffn_output)
        
        # Use last time step for prediction
        last_output = output[-1]
        
        # Quantile predictions
        predictions = {}
        for q in self.quantiles:
            pred = np.dot(last_output, self.output_weights[q])[0]
            predictions[f'q{int(q*100)}'] = np.tanh(pred)  # Bound to [-1, 1]
        
        # Median prediction as primary signal
        signal = predictions['q50']
        
        # Uncertainty (spread between P90 and P10)
        uncertainty = predictions['q90'] - predictions['q10']
        
        # Confidence based on uncertainty (lower uncertainty = higher confidence)
        confidence = 1 - min(abs(uncertainty), 1)
        
        # Position sizing inversely proportional to uncertainty
        position_size = confidence * (1 if abs(signal) > 0.2 else 0)
        
        return {
            'signal': signal,
            'direction': 'LONG' if signal > 0.15 else 'SHORT' if signal < -0.15 else 'NEUTRAL',
            'confidence': confidence,
            'position_size': position_size,
            'quantiles': predictions,
            'uncertainty': uncertainty,
            'model_type': 'Transformer_TFT',
            'timestamp': str(df.index[-1]) if hasattr(df.index[-1], '__str__') else df.index[-1]
        }
    
    def get_attention_map(self) -> Optional[np.ndarray]:
        """
        Get attention weights for interpretability
        Shows which time steps the model focused on
        """
        if not self.attention_weights_history:
            return None
        return np.array(self.attention_weights_history[-1])
    
    def interpret_feature_importance(self) -> Dict[str, float]:
        """
        Return feature importance scores based on variable selection weights
        """
        # Simplified feature importance
        feature_names = [
            'returns', 'ret_6h', 'ret_12h', 'ret_24h', 'ret_48h',
            'vol_12h', 'vol_24h', 'vol_48h', 'volume_norm', 'vw_returns',
            'rsi_proxy', 'macd', 'trend_strength'
        ]
        
        # Return uniform weights as baseline (would be learned in training)
        return {name: 1.0 / len(feature_names) for name in feature_names}


# Model metadata for research documentation
TRANSFORMER_METADATA = {
    "model_name": "CryptoAlpha_Transformer_TFT",
    "architecture": "Temporal Fusion Transformer",
    "components": [
        "Multi-Head Self-Attention",
        "Gated Residual Networks",
        "Variable Selection Networks",
        "Positional Encoding",
        "Quantile Output Layers"
    ],
    "d_model": 64,
    "n_heads": 4,
    "n_layers": 4,
    "lookback_window": "48 bars (8 days)",
    "prediction_type": "Multi-quantile (P10/P50/P90)",
    "key_advantages": [
        "Captures long-range temporal dependencies",
        "Interpretable attention weights",
        "Uncertainty quantification via quantiles",
        "Variable selection for feature importance"
    ],
    "limitations": [
        "Computationally intensive",
        "Requires large training dataset",
        "Risk of overfitting on short sequences",
        "Attention complexity O(nÂ²) with sequence length"
    ],
    "expected_performance": {
        "note": "Transformers typically excel on longer sequences (100+ bars) with abundant training data. May underperform on smaller crypto datasets compared to ensembles."
    }
}


def get_transformer_documentation() -> str:
    """Return formatted model documentation"""
    return json.dumps(TRANSFORMER_METADATA, indent=2)


if __name__ == "__main__":
    print("=" * 80)
    print("TRANSFORMER-BASED PREDICTION MODEL (TFT Architecture)")
    print("=" * 80)
    print("\nModel Metadata:")
    print(get_transformer_documentation())
    print("\n" + "=" * 80)
    print("This model requires training on large historical datasets")
    print("for optimal performance (10,000+ samples recommended)")
    print("=" * 80)
